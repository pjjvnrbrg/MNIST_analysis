{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44376834",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The MNIST dataset is regarded as the \"Hello World!\" of the data scientists journey into machine learning, especially using neural networks for classification problems. It comprises of an extremely well pre-processed and labeled dataset comprising of approxiamtly 70 000 handwritten digits ranging from 0-9. Credit for the dataset goes to Yan LeCun (director of AI research at Facebook),Corinna Cortes and Christopher Burges (yann.lecun.com).\n",
    "\n",
    "\n",
    "\n",
    "We will use the ADAM optimiser for backpropagation and tweak the learing rate as a hyper-parameter. The loss will be determined using the sparse_categorical_cross_entropy since the targets should be 1-hot encoded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14532675",
   "metadata": {},
   "source": [
    "# Problem\n",
    "This is a visual problem, so we can intuitivly assess the accuracy of the model apart from quantifying it through scores and probability. The images were originally stored as 28x28 pixel images in grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1638b",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The objective is to create a model using a neural net to accuratly predict what digit is represented in each image. Specifically the technique used will be supervised machine learning since we have labels associated with the data. The accuracy must be as high as possible using different configurations for the neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affe534",
   "metadata": {},
   "source": [
    "# Method\n",
    "## Input data\n",
    "The images were originally stored as 28x28 pixel data. This is mathematically equivalent to a 28x28 matrix with values ranging from 0-255 (the numeric values corresponding to different shades of gray from black to white) and is how we expect to load the data. A likely approach to handle this will be to “flatten” the matrix into a vector of size 1x784 (the input layer shape of the neural net). Each input neuron would therefore represent the intensity of the grey for a specific pixel in the image. \n",
    "\n",
    "## Forward propagation in hidden layers\n",
    "Each input will be weighted and biased linearly (dot product) to each neuron in the following layer(s) and finally transformed non-linearly using a specific activation function before being forward propagated into the next layer. This is then repeated for n hidden layers. \n",
    "\n",
    "## Output data\n",
    "A 1-hot encoded output lends itself nicely to a classification system rather than a vector of probabilities. The final activation function will be a Softmax function since the output vector should 1x10 in size and be 1-hot encoded. The final activation function will be a Softmax function since the output vector should be 1x10 in size and be 1-hot encoded.\n",
    "\n",
    "## Back propagation\n",
    "The Adaptive Moment Estimation (ADAM) optimiser will be used with different learing rates. The loss will be determined using sparse categorical cross entropy since the targets should be 1-hot encoded.\n",
    "\n",
    "## Steps\n",
    "### Import packages\n",
    "### Helper functions\n",
    "### Load the data\n",
    "### Additional Preprocessing\n",
    "#### Determine validation size\n",
    "#### Standardise or scale the data\n",
    "#### Shuffle the data\n",
    "#### Split into training, validation and test sets\n",
    "### Outline the model(s)\n",
    "### Train the model(s)\n",
    "### Select the best model\n",
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11791b68",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04f8fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:06.149286Z",
     "start_time": "2021-07-28T12:37:03.323056Z"
    }
   },
   "outputs": [],
   "source": [
    "#table and array packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# neural net package\n",
    "import tensorflow as tf\n",
    "\n",
    "# dataset\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b0c72",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369836e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:06.164975Z",
     "start_time": "2021-07-28T12:37:06.150282Z"
    }
   },
   "outputs": [],
   "source": [
    "# a helper function to cast a value into tf.int64\n",
    "def cast_to_tf_integer(x):\n",
    "    # if x is not a tf.int64 object then\n",
    "    if type(x) is not tf.int64:\n",
    "        # cast x as a tf.int64\n",
    "        x = tf.cast(x, tf.int64) \n",
    "    return x\n",
    "\n",
    "# a helper function to cast a value into tf.float32\n",
    "def cast_to_tf_float(x):\n",
    "    # if x is not a tf.float32 object then\n",
    "    if type(x) is not tf.float32:\n",
    "        #cast x as a tf.float32\n",
    "        x = tf.cast(x, tf.float32)\n",
    "    return x\n",
    "\n",
    "# a percent scaling function\n",
    "## input = f(x)\n",
    "## g(x) = f(x)/255\n",
    "## y = g(x) = f(x)/255 = input/255\n",
    "def percent_scale(x, label):\n",
    "    # cast to a tf.float32\n",
    "    x = cast_to_tf_float(x)\n",
    "    # scale by dividing by the max possible value ensuring it is a float\n",
    "    x /= 255.\n",
    "    return x, label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd2151",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3ebeee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:06.498131Z",
     "start_time": "2021-07-28T12:37:06.166970Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "DATA INFO\n",
      "#####################tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    full_name='mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    The MNIST database of handwritten digits.\n",
      "    \"\"\",\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    data_path='C:\\\\Users\\\\pjjvn\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
      "    download_size=11.06 MiB,\n",
      "    dataset_size=21.00 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      ")\n",
      "#####################\n"
     ]
    }
   ],
   "source": [
    "# Load the data for supervised learning into a variable and extract the data information. \n",
    "raw_data, data_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "print(f\"#####################\\nDATA INFO\\n#####################{data_info}\\n#####################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838d2b8",
   "metadata": {},
   "source": [
    "### Additional Preprocessing\n",
    "#### Determine Validation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02f0d41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:06.513331Z",
     "start_time": "2021-07-28T12:37:06.500116Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the training and test sets\n",
    "train_set, test_set = raw_data['train'], raw_data['test']\n",
    "\n",
    "# create a validation set from the train data since it is sufficiently large\n",
    "# use the 10% of the train set size\n",
    "validation_size =0.1*data_info.splits['train'].num_examples\n",
    "\n",
    "# ensure the sizes of the train, validation and test sets are tf.int64\n",
    "train_size = cast_to_tf_integer(data_info.splits['train'].num_examples)\n",
    "validation_size = cast_to_tf_integer(validation_size)\n",
    "test_size = cast_to_tf_integer(data_info.splits['test'].num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ebd5d",
   "metadata": {},
   "source": [
    "#### Standardise or scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c6e071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:06.686779Z",
     "start_time": "2021-07-28T12:37:06.515329Z"
    }
   },
   "outputs": [],
   "source": [
    "# scale using a custom function\n",
    "scaled_train_set = train_set.map(percent_scale)\n",
    "scaled_test_set = test_set.map(percent_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdff1a7",
   "metadata": {},
   "source": [
    "#### Shuffle the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367ddc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:06.734541Z",
     "start_time": "2021-07-28T12:37:06.687821Z"
    }
   },
   "outputs": [],
   "source": [
    "# buffer size\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# shuffle\n",
    "scaled_shuffled_train_set = scaled_train_set.shuffle(\n",
    "                                                    buffer_size = BUFFER_SIZE,\n",
    "                                                    seed=None,\n",
    "                                                    reshuffle_each_iteration=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f15014",
   "metadata": {},
   "source": [
    "#### Split into training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae3b4f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T13:13:10.308189Z",
     "start_time": "2021-07-28T13:13:10.299214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_shuffled_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a67f166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:06.827275Z",
     "start_time": "2021-07-28T12:37:06.735539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "DATA SIZES\n",
      "#####################\n",
      "Training Size: 54000\n",
      "Validation Size: 6000\n",
      "Testing Size: 10000\n",
      "#####################\n"
     ]
    }
   ],
   "source": [
    "# extract the validation set\n",
    "validation_set = scaled_shuffled_train_set.take(validation_size)\n",
    "\n",
    "# exclude the validation set from the training set\n",
    "training_set = scaled_shuffled_train_set.skip(validation_size)\n",
    "\n",
    "# set the new training size and ensure it is a tf.int64\n",
    "training_size = len(training_set)\n",
    "training_size = cast_to_tf_integer(training_size)\n",
    "\n",
    "# set the testing_set\n",
    "testing_set = scaled_test_set\n",
    "testing_size = len(testing_set)\n",
    "\n",
    "print(f\"#####################\\nDATA SIZES\\n#####################\\nTraining Size: {training_size}\\nValidation Size: {validation_size}\\nTesting Size: {testing_size}\\n#####################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a417ff41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T13:06:48.714471Z",
     "start_time": "2021-07-28T13:06:48.697546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SkipDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df398159",
   "metadata": {},
   "source": [
    "#### Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecfb2c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:07.174554Z",
     "start_time": "2021-07-28T12:37:07.163554Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the batch size of the training set\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# batch the traingin set\n",
    "batched_training_set = training_set.batch(BATCH_SIZE)\n",
    "\n",
    "# the model expects the validation in batch form. We need the entire set per\n",
    "# batch so the batch length should be the length of the set to get 1 batch\n",
    "batched_validation_set = validation_set.batch(validation_size)\n",
    "\n",
    "# the model expects the test_set in batch form. We need the entire set per\n",
    "# batch so the batch length should be the length of the set to get 1 batch\n",
    "batched_testing_set = testing_set.batch(testing_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d76010",
   "metadata": {},
   "source": [
    "#### Seperate inputs and targets of the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9787342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:08.533296Z",
     "start_time": "2021-07-28T12:37:07.859021Z"
    }
   },
   "outputs": [],
   "source": [
    "# iter is the syntax for making an object iterable, without \"loading the data\"\n",
    "# next iterates over an interable like in a for loop\n",
    "validation_inputs, validation_targets = next(iter(batched_validation_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b40c80",
   "metadata": {},
   "source": [
    "### Outline the model(s)\n",
    "#### Input layer\n",
    "The input is a 28x28x1 tensor (rank 3) that is to be transformed into a 784x1 vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b0f762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:12.119349Z",
     "start_time": "2021-07-28T12:37:12.107381Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the length\n",
    "input_length = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bef1f9",
   "metadata": {},
   "source": [
    "#### Hidden layers\n",
    "We can use d hidden layers as the depth and w neurons for the width of each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e27940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:12.971199Z",
     "start_time": "2021-07-28T12:37:12.963239Z"
    }
   },
   "outputs": [],
   "source": [
    "# a list of hidden layers widths and depths for 3 models\n",
    "hidden_layer_widths = [50,200]\n",
    "hidden_layer_depths = [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b81cf",
   "metadata": {},
   "source": [
    "#### Output layer\n",
    "The output is a 10x1 vector that is 1-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e708ce99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:13.634201Z",
     "start_time": "2021-07-28T12:37:13.627219Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the length\n",
    "output_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce834b0",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "The optimizer will be an adjustable ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93ccbea6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:16.374720Z",
     "start_time": "2021-07-28T12:37:16.363724Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "LEARNING_RATE = 0.001\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542b885",
   "metadata": {},
   "source": [
    "#### Max Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5cfd6ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:17.463804Z",
     "start_time": "2021-07-28T12:37:17.452805Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b8777",
   "metadata": {},
   "source": [
    "#### Early Stop\n",
    "Early stopping will be used to prevent over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1614c70f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:37:18.423603Z",
     "start_time": "2021-07-28T12:37:18.411608Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "monitor='val_loss',\n",
    "min_delta=0,\n",
    "patience=1,\n",
    "verbose=0,\n",
    "mode='min',\n",
    "restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22935a",
   "metadata": {},
   "source": [
    "#### Model constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6ba6ef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:46:21.186521Z",
     "start_time": "2021-07-28T12:46:21.116677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing a 50x2 model\n",
      "Constructing a 200x2 model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create the model contructs\n",
    "def construct(output_length, optimizer='adam', mode='sequential', flatten_input=True, list_of_widths=[50], list_of_depths=[2]):\n",
    "    dict = {}\n",
    "    #if mode is sequential then\n",
    "    if mode.lower() == 'sequential': \n",
    "        # for the number of widths\n",
    "        for w in list_of_widths:\n",
    "            # for the number of depths \n",
    "            for d in list_of_depths:\n",
    "                # create a label for the dictionary\n",
    "                label = str(w) + 'x' + str(d)\n",
    "                print(f\"Constructing a {label} model\")\n",
    "                # instantiate the model object\n",
    "                if flatten_input is True:\n",
    "                    model = tf.keras.Sequential([\n",
    "                        # input layer:\n",
    "                        #           flatten the tensor into a vector using an inbuilt method of tensor flow\n",
    "                        tf.keras.layers.Flatten(input_shape=(28,28,1))])\n",
    "                else:\n",
    "                    model = tf.keras.Sequential()\n",
    "                # for d layers\n",
    "                for x in range(d):\n",
    "                    # add a hidden layer of width w\n",
    "                    model.add(tf.keras.layers.Dense(w,activation = 'relu'))\n",
    "                # add the output layer\n",
    "                model.add(tf.keras.layers.Dense(output_length,activation = 'softmax'))\n",
    "                # choose the optimizer and loss functions\n",
    "                model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "                # save in the dicitonary\n",
    "                dict[label] = model\n",
    "    return dict\n",
    "\n",
    "model_dict = construct(output_length=output_length, optimizer = adam_optimizer, mode='sequential', flatten_input=True, list_of_widths=hidden_layer_widths, list_of_depths=hidden_layer_depths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046dc4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T12:16:43.284694Z",
     "start_time": "2021-07-23T12:16:43.272698Z"
    }
   },
   "source": [
    "### Fit the models - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc66b231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:46:26.364722Z",
     "start_time": "2021-07-28T12:46:26.355718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n",
      "<TakeDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(batched_training_set)\n",
    "print(validation_set)\n",
    "print(batched_testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2be2f36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:46:26.839255Z",
     "start_time": "2021-07-28T12:46:26.828253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'50x2': <tensorflow.python.keras.engine.sequential.Sequential at 0x26d726ab6d0>,\n",
       " '200x2': <tensorflow.python.keras.engine.sequential.Sequential at 0x26d726b49a0>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aea798a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:47:03.365647Z",
     "start_time": "2021-07-28T12:46:27.843763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "50x2 model\n",
      "#####################\n",
      "Epoch 1/100\n",
      "540/540 - 2s - loss: 0.2808 - accuracy: 0.9163 - val_loss: 0.1683 - val_accuracy: 0.9485\n",
      "Epoch 2/100\n",
      "540/540 - 1s - loss: 0.1447 - accuracy: 0.9565 - val_loss: 0.1342 - val_accuracy: 0.9597\n",
      "Epoch 3/100\n",
      "540/540 - 1s - loss: 0.1159 - accuracy: 0.9655 - val_loss: 0.1054 - val_accuracy: 0.9668\n",
      "Epoch 4/100\n",
      "540/540 - 1s - loss: 0.0953 - accuracy: 0.9713 - val_loss: 0.0956 - val_accuracy: 0.9712\n",
      "Epoch 5/100\n",
      "540/540 - 1s - loss: 0.0838 - accuracy: 0.9747 - val_loss: 0.0889 - val_accuracy: 0.9748\n",
      "Epoch 6/100\n",
      "540/540 - 1s - loss: 0.0737 - accuracy: 0.9774 - val_loss: 0.0745 - val_accuracy: 0.9793\n",
      "Epoch 7/100\n",
      "540/540 - 1s - loss: 0.0632 - accuracy: 0.9803 - val_loss: 0.0720 - val_accuracy: 0.9783\n",
      "Epoch 8/100\n",
      "540/540 - 1s - loss: 0.0566 - accuracy: 0.9831 - val_loss: 0.0632 - val_accuracy: 0.9788\n",
      "Epoch 9/100\n",
      "540/540 - 1s - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.0623 - val_accuracy: 0.9817\n",
      "Epoch 10/100\n",
      "540/540 - 1s - loss: 0.0450 - accuracy: 0.9860 - val_loss: 0.0531 - val_accuracy: 0.9835\n",
      "Epoch 11/100\n",
      "540/540 - 1s - loss: 0.0410 - accuracy: 0.9875 - val_loss: 0.0499 - val_accuracy: 0.9837\n",
      "Epoch 12/100\n",
      "540/540 - 1s - loss: 0.0359 - accuracy: 0.9892 - val_loss: 0.0496 - val_accuracy: 0.9847\n",
      "Epoch 13/100\n",
      "540/540 - 1s - loss: 0.0339 - accuracy: 0.9899 - val_loss: 0.0422 - val_accuracy: 0.9875\n",
      "Epoch 14/100\n",
      "540/540 - 1s - loss: 0.0298 - accuracy: 0.9910 - val_loss: 0.0469 - val_accuracy: 0.9850\n",
      "\n",
      "\n",
      "#####################\n",
      "200x2 model\n",
      "#####################\n",
      "Epoch 1/100\n",
      "540/540 - 2s - loss: 0.1948 - accuracy: 0.9413 - val_loss: 0.1026 - val_accuracy: 0.9672\n",
      "Epoch 2/100\n",
      "540/540 - 2s - loss: 0.0770 - accuracy: 0.9761 - val_loss: 0.0764 - val_accuracy: 0.9763\n",
      "Epoch 3/100\n",
      "540/540 - 2s - loss: 0.0507 - accuracy: 0.9844 - val_loss: 0.0530 - val_accuracy: 0.9830\n",
      "Epoch 4/100\n",
      "540/540 - 2s - loss: 0.0357 - accuracy: 0.9891 - val_loss: 0.0348 - val_accuracy: 0.9893\n",
      "Epoch 5/100\n",
      "540/540 - 3s - loss: 0.0277 - accuracy: 0.9911 - val_loss: 0.0376 - val_accuracy: 0.9885\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each model in the dictionary\n",
    "for label in model_dict:\n",
    "    # fit the model\n",
    "    print(f\"#####################\\n{label} model\\n#####################\")\n",
    "    model_dict[label].fit(\n",
    "        batched_training_set,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        callbacks=[early_stop],\n",
    "        validation_data=(validation_inputs,validation_targets),\n",
    "        verbose=2)\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7457b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T14:01:55.935283Z",
     "start_time": "2021-07-23T14:01:55.916593Z"
    }
   },
   "source": [
    "### Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f7ed9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T12:47:04.694768Z",
     "start_time": "2021-07-28T12:47:03.367643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "50x2 model\n",
      "#####################\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0972 - accuracy: 0.9738\n",
      "Test Loss: 0.09716800600290298'\n",
      "Test Accuracy: 0.973800003528595\n",
      "C:\\Users\\pjjvn\\Google Drive\\Programming\\MNIST_analysis\n",
      "#####################\n",
      "200x2 model\n",
      "#####################\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0673 - accuracy: 0.9796\n",
      "Test Loss: 0.06730654090642929'\n",
      "Test Accuracy: 0.9796000123023987\n",
      "C:\\Users\\pjjvn\\Google Drive\\Programming\\MNIST_analysis\n"
     ]
    }
   ],
   "source": [
    "# for each model in the dictionary\n",
    "for label in model_dict:\n",
    "    # extract the model\n",
    "    model = model_dict[label]\n",
    "    # test the model\n",
    "    print(f\"#####################\\n{label} model\\n#####################\")\n",
    "    test_loss, test_accuracy = model.evaluate(batched_testing_set)\n",
    "    print(f\"Test Loss: {test_loss}'\\nTest Accuracy: {test_accuracy}\")\n",
    "    # save the model\n",
    "    import os\n",
    "    working_directory = os.getcwd()\n",
    "    print(working_directory)\n",
    "    model.save_weights(working_directory+f'\\\\{label}_model_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b1945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
